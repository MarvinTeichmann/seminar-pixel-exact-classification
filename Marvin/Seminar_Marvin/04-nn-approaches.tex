%!TEX root = vorlage.tex
% Marvin Teichmann

%TODO: Type of MLP
\section{Neural Networks for Segmentation}\label{sec:fcn}

After the overwhelming successes of \Glspl{DCNN} in image classification, there as been a lot of effort to apply this models to further computer vision tasks. Early ideas include the use of \Glspl{CNN} based classifiers in combination with traditional classifiers \cite{RNN}. Other authors used the idea descripted in \cref{sec:tasks} to tackle segmentation as pixel-wise classification problem using a sliding-window approach together with a classification network \cite{fast_scanning}, \cite{overfeat}, \cite{bktt} \cite{highly}. These authors profit from the inherent sliding window efficenty of \Glspl{CNN}, descripted in \cref{sec:convnet}. A recent break-trough has been archieved with the novel \gls{FCN} \cite{fcn} architectures. \Gls{FCN} are an architecture specifically designed for Semantic Segmentation. \Gls{FCN} combine the sliding window efficenty with a deconvolution architecture for upsampling and a transfer learning approach. \glspl{FCN} and there deeper successors \cite{CRF1}, \cite{deconv1} \cite{googleSeg} are currently the state-of-the art in several semantic segmentation benchmarks. We will descripe the mechanics behind \glspl{FCN} in detail in \cref{sec:fcn_detail}.


\iffalse

As described in \Cref{sec:tasks} Semantic Segmentation can be views as a spatial version of classification, where a classification model can be transfered into a segmentation model using a sliding window approach. In \glspl{CNN} sliding window can be carried out very efficiently. Most CNN based segmentation approaches are using this insight to build models on the shoulders of AlexNet and its deeper successors. 

\fi

\subsection{Sliding Window efficiency in CNNs} \label{sec:convnet}

Opposed to other classification approaches ConvNets are inherently effecient when applied in sliding window fashion. Their translation invariant structure allows to benifit from computation on overlapping patches on the images. Of high practical relevants is also, that the result itselfe will be a ConvNet, that means any ConvNet $C$ can easily be transformed in a ConvNet $C'$, whose output is equal to applying $C$ in sliding window fashion. This idea can hence be very efficently implemented in any ConvNet framework, without much efford. The only downside is, that $C'$ will have a stride $s$ equal to the product of all strides in $C$.

The reason for the efficienty is, that translation invariant computing is computational traceable. Let $F, G$ function, computing a layer as defined in \cref{sec:cnn_not}. Let $f,g$ kernels corresponding with sizes $k,k'$ and stride $s,s'$, corresponding to $F, G$ respectively. Than $F \circ G$ is obtained by the kernel $f \circ g$, which has kernel size $k + (k-1) s'$ using stride $s \cdot s'$. For networks only consisting of convolution and pooling layers one can therefore simple increase the size of the input layer at evaluation time. The output will be equivalent to apply the original network in sliding-window fashion. Common ConvNet architectures typically have one or more fully-connected layer producing the final classification output. This layers can be replaced by convolutional layers using the trick descripted in \cref{sec:fully_connected}. 

The main downside of this approach is the stride of the overall output stride. The output image of the transformed ConvNet $C'$ will be an low resolution image. The input image will be downsampled by a factor of $s$ corresponding to the product of all strides beeing applied in $C'$. On most network architectures $s$ becomes quite large, e.g. $32$ on VGG16, a network using pooling very cautiously. 

To avoid downsampling while still profiting from the sliding window efficiently of \glspl{CNN} it is possible to use shift-and-stitch. Each layer which is associated with an stride $s$ is feeded with $s^2$ shifted versions of the input. (Each with a different shift in $x$ or $y$ dimension). The output can than be stitched together in order to obtain an image of original resolution. The result is than equivalent in applying sliding-window with stride 1. However the computational advatage of applying strided pooling is lost in the procedure (while the model advantage remains). Fast-scanning \cite{fast_scanning} descripes a trick to efficiently perform this computation.  This idea is used in several publications \cite{overfeat,huval}.


\subsection{FCN} \label{sec:fcn_detail}

The \glspl{FCN} \cite{fcn} Architecture builds up on the ideas presented of \cref{sec:convnet}. Similar to earlier approaches they are using existing classification networks and transform them into segmentation networks using the inherent sliding-window efficientcy of ConvNets. However opposed to earlier approach they are not trying to avoid downsampling as part of the progress, but they are using a trainable upsampling layer to archive high resolution output. Further ingredients of their approach is a skip-architecture to  preserve fine-grained information and a transfer-learning approach making it possible to train a very deep net.


\subsubsection{Deconvolution} Todo: detailed explanation of deconvolution



\subsubsection{Skip-Architecture}



\subsubsection{Transfer Learning} One of the strengthes of the FCN approach is the use of transfer learning. Training is done in two steps. First the classification network is done using weakly annotated data (i.e. classification labels). Afterwards the last layer of the architecture is replaced by a deconvolution layer. Only than the Network is fine-tuned on segmentation data. Usually weakly labeled data is much cheaper to obtain than fully labeled segmentation data making this approach very attractive for a lot of applications.


In the publication several the FCN approach was applied to AlexNet, VGG16 and GoogLeNet. The best results where achieved on VGG16. A possible explanation for this is, that VGG16 uses stride and pooling most cautiously perserving spatial information best.




\subsection{Extensions of FCN}

Several extensions of FCN have been proposed. All of are based on VGG16. The main differentses between the approaches are how the how upsambling is performed. 

Several Authors have combined FCNs with CRFs very succesfully. Zheng et. al \cite{CRF1} and Chen et at. \cite{CRF2} improved the results of Long et. al \cite{fcn} by applying \gls{CRF} on top of the FCN architecture. 

\cite{deconv1} and \cite{segnet} proposed a slightly different approach. The designed a deep decoding network to performe the upsampling. Where each layer of the decoding network corresponds to a pooling layer of the VGG network. The upsampling itselfe is not trained but computed directly using the max pooling indezies. Trained convolution layers are used between the upsampling operations to refine the results. The main downside of this approaches is, that the networks need a large amount of strong labeled data as they are fully trained end-to-end. This problem is relaxed in \cite{decoupled}, by introducing transfer-learning to deep deconvolution networks.





