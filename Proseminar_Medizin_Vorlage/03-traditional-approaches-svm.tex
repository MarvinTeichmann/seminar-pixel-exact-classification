%!TEX root = vorlage.tex

\subsection{SVMs}\label{subsec:trad-SVM}%

\Glspl{SVM} are well-studied binary classifiers which can be described by four
central ideas. For those ideas, the training data is represented as
$(\mathbf{x}_i, y_i)$ where $\mathbf{x}_i$ is the feature vector and $y_i \in
\Set{-1, 1}$ the binary label for training example $i \in \Set{1, \dots, m}$.


\begin{enumerate}
    \item If data is linearly separable, it can be separated by a hyperplane.
          There is one hyperplane which maximizes the distance to the
          datapoints. This hyperplane should be taken:\\
          \begin{equation*}
          \begin{aligned}
              \minimize_{\mathbf{w}, b}\,&\frac{1}{2} \|\mathbf{w}\|^2\\
              \text{s.t. }& \forall_{i=1}^m y_i \cdot \underbrace{(\langle \mathbf{w}, \mathbf{x}_i\rangle + b)}_{\mathclap{\sgn \text{ applied to this gives the classification}}} \geq 1
          \end{aligned}
          \end{equation*}
    \item Even if the underlying process which generates the features for the
          two classes is linearly separable, noise can make the data not
          separable. The introduction of \textit{slack variables} to relax the
          requirement of linear separability solves this problem. The trade-off
          between accepting some errors and a more complex model is weighted by
          a parameter $C \in \mathbb{R}_0^+$. The bigger $C$, the more errors
          are accepted. The new optimization problem is:
          \begin{equation*}
          \begin{aligned}
              \minimize_{\mathbf{w}}\,&\frac{1}{2} \|\mathbf{w}\|^2 + C \cdot \sum_{i=1}^m \xi_i\\
              \text{s.t. }& \forall_{i=1}^m y_i \cdot (\langle \mathbf{w}, \mathbf{x}_i\rangle + b) \geq 1 - \xi_i
          \end{aligned}
          \end{equation*}
    \item The primal problem is to find the normal vector $\mathbf{w}$ and the
          bias $b$. The dual problem is to express $\mathbf{w}$ as a linear
          combination of the training data $\mathbf{x}_i$:
          \[\mathbf{w} = \sum_{i=1}^m \alpha_i y_i \mathbf{x}_i\]
          where $y_i \in \Set{-1, 1}$ represents the class of the training
          example and $\alpha_i$ are Lagrange multipliers. The usage of
          Lagrange multipliers is explained with some examples
          in~\cite{smithlagrange}. The usage of the Lagrange multipliers
          $\alpha_i$ changes the optimization problem depend on the
          $\alpha_i$ which are weights for the feature vectors. It turns
          out that most $\alpha_i$ will be zero. The non-zero weighted vectors
          are called \textit{support vectors}.

          The optimization problem is now, according to~\cite{burges1998tutorial}:
          \begin{equation*}
          \begin{aligned}
              \maximize_{\mathbf{w}}\,& \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j y_i y_j \langle \mathbf{x}_i, \mathbf{x}_j \rangle\\
              \text{s.t. } & \forall_{i=1}^m 0 \leq \alpha_i \leq C\\
              \text{s.t. } & \sum_{i=1}^m \alpha_i y_i = 0
          \end{aligned}
          \end{equation*}
    \item Not every dataset is linearly separable. This problem is approached
          by transforming the feature vectors $\mathbf{x}$ with a non-linear
          mapping $\Phi$ into a higher dimensional (probably
          $\infty$-dimensional) space. As the feature vectors $\mathbf{x}$
          are only used within scalar product
          $\langle \mathbf{x}_i, \mathbf{x}_j \rangle$, it is not necessary to
          do the transformation. It is enough to do the calculation
          \[K(\mathbf{x}_i, \mathbf{x}_j) = \langle \mathbf{x}_i, \mathbf{x}_j \rangle\]

          This function $K$ is called a \textit{kernel}. The idea of never
          explicitly transforming the vectors $\mathbf{x}_i$ to the higher
          dimensional space is called the \textit{kernel trick}. Common kernels
          include the polynomial kernel
          \[K_P(\mathbf{x}_i, \mathbf{x}_j) = (\langle \mathbf{x}_i, \mathbf{x}_j \rangle + r)^p\]
          of degree $p$ and coefficient $r$, the Gaussian \gls{RBF} kernel
          \[K_{\text{Gauss}}(\mathbf{x}_i, \mathbf{x}_j) = e^{\frac{-\gamma\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2 \sigma^2}}\]
          and the sigmoid kernel
          \[K_{\text{tanh}}(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\gamma \langle \mathbf{x}_i, \mathbf{x}_j \rangle - r)\]
          where the parameter $\gamma$ determines how much influence single
          training examples have.
\end{enumerate}

The described \glspl{SVM} can only distinguish between two classes. Common
strategies to expand those binary classifiers to multi-class classification is
the \textit{one-vs-all} and the \textit{one-vs-one} strategy. In the one-vs-all
strategy $n$ classifiers have to be trained which can distinguish one of the $n$
classes against all other classes. In the one-vs-one strategy $\frac{n^2 - n}{2}$
classifiers are trained; one classifier for each pair of classes.

A detailed description of \glspl{SVM} can be found in~\cite{burges1998tutorial}.

\Glspl{SVM} are used by \cite{yang2012layered} on the 2009 and 2010 PASCAL
segmentation challenge~\cite{everingham2010pascal}. They did not hand their
classifier in to the challenge itself, but calculated an average rank~of~7
among the different categories.

\cite{felzenszwalb2010object} also used an \gls{SVM} based method with \gls{HOG}
features and achieved the \nth{7}~rank in the 2010 PASCAL segmentation
challenge by mean accuracy. It needs about \SI{2}{\second} on a
\SI{2.8}{\giga\hertz} 8-core Intel processor.
